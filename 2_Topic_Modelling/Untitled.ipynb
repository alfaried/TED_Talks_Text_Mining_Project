{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets list of stop words and stemmer object\n",
    "stop_list = nltk.corpus.stopwords.words(\"english\")\n",
    "stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts text files into a corpus\n",
    "test_corpus = nltk.corpus.PlaintextCorpusReader(\"./TrainTest_Transcripts/Test/\", \".+\\.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts corpus into a list of documents\n",
    "fids = test_corpus.fileids()\n",
    "docs1 = []\n",
    "for fid in fids:\n",
    "    doc_raw = test_corpus.raw(fid)\n",
    "    doc = nltk.word_tokenize(doc_raw)\n",
    "    docs1.append(doc)\n",
    "docs2 = [[w.lower() for w in doc] for doc in docs1]\n",
    "docs3 = [[w for w in doc if re.search('^[a-z]+$', w)] for doc in docs2]\n",
    "docs4 = [[w for w in doc if w not in stop_list] for doc in docs3]\n",
    "test_docs = [[stemmer.stem(w) for w in doc] for doc in docs4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dictionary for bag-of-words purposes\n",
    "test_dictionary = gensim.corpora.Dictionary(test_docs)\n",
    "\n",
    "# Convert the list of documents into vectors\n",
    "test_vecs = [train_dictionary.doc2bow(doc) for doc in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = r'/Library/NLTK/mallet/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing of external package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LDA_Module import LDA, Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing test dataset\n",
      "Loading corpus..\n",
      "Loading docs..\n",
      "Loading dictionary..\n",
      "Loading vecs..\n"
     ]
    }
   ],
   "source": [
    "test_transcript = Preprocess.Dataset(\"./TrainTest_Transcripts/Test/\", \"test\")\n",
    "test_vecs = test_transcript.get_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_obj = LDA.LDA_Mallet(mallet_path)\n",
    "lda_obj.get_pretrained_model()\n",
    "topic_distribution_list = lda_obj.predict_topics_for_vecs(test_vecs)\n",
    "dominant_topics_list = lda_obj.get_dominant_topic_for_vecs(topic_distribution_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = test_transcript.get_corpus()\n",
    "fids = test_corpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_hash = {}\n",
    "for index in range(0, len(fids)):\n",
    "    transcript_id = fids[index].split(\".\")[0].split(\"_\")[1]\n",
    "    test_predict_hash[transcript_id] = dominant_topics_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " 0.13588820301783266,\n",
       " 'food, plant, eat, grow, farm, feed, farmer, cook, agricultur, call')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict_hash[\"1008\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(\"./TrainTest_Transcripts/Test/trasncript_1008.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
