{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Document Clustering on Transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets in\n",
    "metadata_dataset = pd.read_csv(\"../Raw_Dataset/ted_main.csv\", keep_default_na=False, na_values=[\"\"])\n",
    "transcript_dataset = pd.read_csv(\"../Raw_Dataset/transcripts.csv\", keep_default_na=False, na_values=[\"\"])\n",
    "\n",
    "# Load the reference dataset\n",
    "ref_dataset = pd.read_csv(\"../Reference_Dataset/url_w_label.csv\", keep_default_na=False, na_values=[\"\"])\n",
    "\n",
    "# Merge the two datasets into one large one\n",
    "combined_dataset = pd.merge(left=metadata_dataset, right=transcript_dataset, left_on=\"url\", right_on=\"url\")\n",
    "\n",
    "# Merge the two dataset together, to get labels\n",
    "combined_w_label_dataset = pd.merge(left=combined_dataset, right=ref_dataset, left_on=\"url\", right_on=\"url\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.2.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept rows with speaker == 1\n",
    "combined_w_label_w_1_speaker_dataset = combined_w_label_dataset[ combined_w_label_dataset[\"num_speaker\"] == 1 ]\n",
    "\n",
    "# Remove rows labeled as music or conversation\n",
    "music_removed = combined_w_label_w_1_speaker_dataset[ combined_w_label_w_1_speaker_dataset[\"music\"] != 1 ]\n",
    "cleaned_dataset = music_removed[ music_removed[\"conversation\"] != 1 ] \n",
    "\n",
    "# Remove music and conversation column\n",
    "cleaned_dataset = cleaned_dataset.drop(columns=\"music\")\n",
    "cleaned_dataset = cleaned_dataset.drop(columns=\"conversation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Processed_Data folder\n",
    "cleaned_dataset.to_csv(\"../Processed_Dataset/cleaned_dataset.csv\", index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.2.3 Data Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split 80% train and 20% test data\n",
    "train, test = train_test_split(cleaned_dataset, test_size=0.2, train_size=0.8)\n",
    "\n",
    "# Further split the train dataset into 80% train and 20% validate\n",
    "train, validate = train_test_split(train, test_size=0.2, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
