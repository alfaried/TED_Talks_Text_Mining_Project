{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Document Clustering with Key Words Extraction (TF-IDF Weightage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "import k_means\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.cluster import KMeans \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Train and Test Dataset from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"./TrainTest_Dataset/train_dataset.csv\", keep_default_na=False, na_values=[\"\"])\n",
    "test_dataset = pd.read_csv(\"./TrainTest_Dataset/test_dataset.csv\", keep_default_na=False, na_values=[\"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Train and Test Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Training Corpus and Pre-Processing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = nltk.corpus.PlaintextCorpusReader(\"./TrainTest_Transcripts/Train\", \".+\\.txt\")\n",
    "train_fids = train_corpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing List of Stop Words and Stemming using NLTK Library \n",
    "stop_list = nltk.corpus.stopwords.words(\"english\")\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "trainData_tokenized = [train_corpus.words(fid) for fid in train_fids] \n",
    "trainData_lowerCase = [[w.lower() for w in doc] for doc in trainData_tokenized]\n",
    "trainData_removedPunct = [[w for w in doc if re.search('^[a-z]+$', w)] for doc in trainData_lowerCase]\n",
    "trainData_removedStopwords = [[w for w in doc if w not in stop_list] for doc in trainData_removedPunct]\n",
    "trainData_processed = [[stemmer.stem(w) for w in doc] for doc in trainData_removedStopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Training Corpus and Pre-Processing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = nltk.corpus.PlaintextCorpusReader(\"./TrainTest_Transcripts/Test\", \".+\\.txt\")\n",
    "test_fids = test_corpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing List of Stop Words and Stemming using NLTK Library \n",
    "testData_tokenized = [test_corpus.words(fid) for fid in test_fids] \n",
    "testData_lowerCase = [[w.lower() for w in doc] for doc in testData_tokenized]\n",
    "testData_removedPunct = [[w for w in doc if re.search('^[a-z]+$', w)] for doc in testData_lowerCase]\n",
    "testData_removedStopwords = [[w for w in doc if w not in stop_list] for doc in testData_removedPunct]\n",
    "testData_processed = [[stemmer.stem(w) for w in doc] for doc in testData_removedStopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Train Data into Sparse Vector with TF-IDF Weightage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word dictionary \n",
    "dict_train = gensim.corpora.Dictionary(trainData_processed)\n",
    "\n",
    "# Bag of Words Vector \n",
    "bowVec_train = [dict_train.doc2bow(doc) for doc in trainData_processed] \n",
    "\n",
    "# TF*IDF Model for Training Data \n",
    "tfidfModel_train = gensim.models.TfidfModel(bowVec_train) \n",
    "\n",
    "# Sparse Vector with TFIDF Weightage\n",
    "SparseVec_train = [tfidfModel_train[vec] for vec in bowVec_train] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.tfidfmodel.TfidfModel at 0x1a451322d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfModel_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Top-N Weighted Words from each Document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract top-N words from Document\n",
    "def Extract_Doc_Top_N_Tfidf_Words(orginal_list, TopNWords):\n",
    "    # Sort each document word's TF-IDF weightage (list of tuples) in descending order \n",
    "    updated_list = copy.deepcopy(orginal_list)\n",
    "    for index in range(0,len(updated_list)):\n",
    "        updated_list[index].sort(key = lambda x: x[1], reverse=True)\n",
    "        updated_list[index] = updated_list[index][:TopNWords] # Extract the top 100 weighted words  \n",
    "    return updated_list  \n",
    "\n",
    "# Function to create new Dictionary based on the new Sparse Vector\n",
    "def Create_New_Dictionary(sparseVec, original_dict):\n",
    "    # Converting Bag of Words ID in sparseVec into a list of Tokens \n",
    "    # E.g. list of \"list of string\" e.g. [['SMU','SU'],['Graduate','Wow']]\n",
    "    tokenized_list = [[original_dict.get(tup[0]) for tup in doc] for doc in sparseVec] \n",
    "    return gensim.corpora.Dictionary(tokenized_list)\n",
    "\n",
    "# Function to create new Sparse Vector based on the new dictionary \n",
    "def Create_New_Sparse_Vector(sparseVec, new_dictionary, old_dictionary):    \n",
    "    # Inverting Dictionary Key and Value for new_dictionary for easier retrieval\n",
    "    # From [{word_id : word}] to [{word : word_id}]\n",
    "    inverted_dict = dict(zip(new_dictionary.values(), new_dictionary.keys()))\n",
    "    new_sparseVec = []\n",
    "    for doc in sparseVec:\n",
    "        new_doc = []\n",
    "        for tup in doc:\n",
    "            word = old_dictionary.get(tup[0]) # retrieve word from old_dictionary          \n",
    "            new_doc.append((inverted_dict.get(word),tup[1]))\n",
    "        new_sparseVec.append(new_doc)\n",
    "    return new_sparseVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse Vector with top-N TF-IDF words from each document. \n",
    "# Keep in mind that this contains Word ID of the previous Dictionary  \n",
    "extracted_SparseVec_train = Extract_Doc_Top_N_Tfidf_Words(SparseVec_train, 50) \n",
    "\n",
    "# Creating new set of Dictionary  \n",
    "new_dict_train = Create_New_Dictionary(extracted_SparseVec_train, dict_train)\n",
    "\n",
    "# Updating Sparse Vector's word ID \n",
    "new_SparseVec_train = Create_New_Sparse_Vector(extracted_SparseVec_train, new_dict_train, dict_train)\n",
    "\n",
    "# Transforming Sparse Vector into np array to find optimal K \n",
    "train_nparray = gensim.matutils.corpus2dense(new_SparseVec_train, len(new_dict_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31764\n"
     ]
    }
   ],
   "source": [
    "print(len(new_dict_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 Data Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3.1 Identifying Optimal K \n",
    "Adapted from: https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-for-k-means-clustering-14f27070048f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running:  25\n",
      "running:  26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6dee51e6649e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"running: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mkm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mkm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_nparray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mSum_of_squared_distances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    970\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    973\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_squared_norms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_squared_norms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 random_state=random_state)\n\u001b[0m\u001b[1;32m    382\u001b[0m             \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[0;34m(X, sample_weight, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;31m# init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     centers = _init_centroids(X, n_clusters, init, random_state=random_state,\n\u001b[0;32m--> 437\u001b[0;31m                               x_squared_norms=x_squared_norms)\n\u001b[0m\u001b[1;32m    438\u001b[0m     \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_init_centroids\u001b[0;34m(X, k, init, random_state, x_squared_norms, init_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         centers = _k_init(X, k, random_state=random_state,\n\u001b[0;32m--> 749\u001b[0;31m                           x_squared_norms=x_squared_norms)\n\u001b[0m\u001b[1;32m    750\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'random'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0mseeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_k_init\u001b[0;34m(X, n_clusters, x_squared_norms, random_state, n_local_trials)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Compute distances to center candidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         distance_to_candidates = euclidean_distances(\n\u001b[0;32m--> 118\u001b[0;31m             X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Decide which candidate is the best\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0mpaired_distances\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0mbetweens\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0mof\u001b[0m \u001b[0melements\u001b[0m \u001b[0mof\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \"\"\"\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;31m# If norms are passed as float32, they are unused. If arrays are passed as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m                         estimator=estimator)\n\u001b[1;32m    113\u001b[0m         Y = check_array(Y, accept_sparse='csr', dtype=dtype,\n\u001b[0;32m--> 114\u001b[0;31m                         estimator=estimator)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprecomputed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# safely to reduce dtype induced overflows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mis_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'fc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_float\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_safe_accumulator_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_float\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36m_safe_accumulator_op\u001b[0;34m(op, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m     \"\"\"\n\u001b[1;32m    685\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2182\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sum_of_squared_distances 25 - 50 \n",
    "Sum_of_squared_distances = [] \n",
    "K = range(25,50)\n",
    "for k in K:\n",
    "    print(\"running: \", k)\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(train_nparray)\n",
    "    Sum_of_squared_distances.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running:  50\n",
      "running:  51\n",
      "running:  52\n",
      "running:  53\n",
      "running:  54\n",
      "running:  55\n",
      "running:  56\n",
      "running:  57\n",
      "running:  58\n",
      "running:  59\n",
      "running:  60\n"
     ]
    }
   ],
   "source": [
    "# Sum_of_squared_distances 50 - 60 \n",
    "Sum_of_squared_distances_50_60 = [] \n",
    "K = range(50,61)\n",
    "for k in K:\n",
    "    print(\"running: \", k)\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(train_nparray)\n",
    "    Sum_of_squared_distances_50_60.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running:  1\n",
      "running:  2\n",
      "running:  3\n",
      "running:  4\n",
      "running:  5\n",
      "running:  6\n",
      "running:  7\n",
      "running:  8\n",
      "running:  9\n",
      "running:  10\n",
      "running:  11\n",
      "running:  12\n",
      "running:  13\n",
      "running:  14\n",
      "running:  15\n",
      "running:  16\n",
      "running:  17\n",
      "running:  18\n",
      "running:  19\n",
      "running:  20\n",
      "running:  21\n",
      "running:  22\n",
      "running:  23\n",
      "running:  24\n"
     ]
    }
   ],
   "source": [
    "# Sum_of_squared_distances 25 - 50 \n",
    "Sum_of_squared_distances_1_25 = [] \n",
    "K = range(1,25)\n",
    "for k in K:\n",
    "    print(\"running: \", k)\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(train_nparray)\n",
    "    Sum_of_squared_distances_1_25.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running:  1\n",
      "running:  2\n",
      "running:  3\n",
      "running:  4\n",
      "running:  5\n",
      "running:  6\n",
      "running:  7\n",
      "running:  8\n",
      "running:  9\n",
      "running:  10\n",
      "running:  11\n",
      "running:  12\n",
      "running:  13\n",
      "running:  14\n",
      "running:  15\n",
      "running:  16\n",
      "running:  17\n",
      "running:  18\n",
      "running:  19\n",
      "running:  20\n",
      "running:  21\n",
      "running:  22\n",
      "running:  23\n",
      "running:  24\n",
      "running:  25\n",
      "running:  26\n",
      "running:  27\n",
      "running:  28\n",
      "running:  29\n",
      "running:  30\n",
      "running:  31\n",
      "running:  32\n",
      "running:  33\n",
      "running:  34\n",
      "running:  35\n",
      "running:  36\n",
      "running:  37\n",
      "running:  38\n",
      "running:  39\n",
      "running:  40\n",
      "running:  41\n",
      "running:  42\n",
      "running:  43\n"
     ]
    }
   ],
   "source": [
    "Sum_of_squared_distances_1_70_top50 = [] \n",
    "K = range(1,70)\n",
    "for k in K:\n",
    "    print(\"running: \", k)\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(train_nparray)\n",
    "    Sum_of_squared_distances_1_70_top50.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances_combined = Sum_of_squared_distances_1_25 + Sum_of_squared_distances + Sum_of_squared_distances_50_60\n",
    "K = range(1,61)\n",
    "plt.plot(K, Sum_of_squared_distances_combined, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 words\n",
    "SSE_1_to_60 = [1200.529,\n",
    " 1196.3323342672006,\n",
    " 1194.323539188254,\n",
    " 1190.3827957081608,\n",
    " 1189.5341851201583,\n",
    " 1185.2916176838387,\n",
    " 1186.2149705667132,\n",
    " 1178.2161681608866,\n",
    " 1183.124056918208,\n",
    " 1179.7356436899272,\n",
    " 1177.1693003649445,\n",
    " 1171.1892138455205,\n",
    " 1172.1641868334086,\n",
    " 1169.621470092471,\n",
    " 1171.169605025566,\n",
    " 1167.7569279579495,\n",
    " 1162.190902994428,\n",
    " 1161.3990270622664,\n",
    " 1166.6206731896218,\n",
    " 1158.9378446083288,\n",
    " 1158.555941227098,\n",
    " 1157.8717206268443,\n",
    " 1152.6693726678818,\n",
    " 1155.0102177434894,\n",
    " 1151.1929803453152,\n",
    " 1150.7755448315022,\n",
    " 1151.4212970926314,\n",
    " 1149.5556119536752,\n",
    " 1148.7469748856402,\n",
    " 1144.3938185311195,\n",
    " 1140.916625454774,\n",
    " 1145.3388127402693,\n",
    " 1135.314411028918,\n",
    " 1142.1188291490348,\n",
    " 1138.3379662130476,\n",
    " 1134.0708192422483,\n",
    " 1137.4154360797877,\n",
    " 1131.2364474072092,\n",
    " 1132.2894346857647,\n",
    " 1130.9126004298303,\n",
    " 1128.8152346117517,\n",
    " 1125.6459607688628,\n",
    " 1123.0622207715935,\n",
    " 1122.6032038001406,\n",
    " 1121.420629927655,\n",
    " 1122.5251307805763,\n",
    " 1120.778991419416,\n",
    " 1118.6097317436756,\n",
    " 1119.7536889152573,\n",
    " 1117.3965153013648,\n",
    " 1117.088696365621,\n",
    " 1111.6940503527014,\n",
    " 1110.276092021481,\n",
    " 1111.6020542431054,\n",
    " 1106.6224304310563,\n",
    " 1102.7490013282088,\n",
    " 1103.0655575868923,\n",
    " 1105.021530971285,\n",
    " 1102.2340467837746,\n",
    " 1101.5132227916567]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3.2 Clustering Corpus based on Optimal-K of 48\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(new_dict_train.token2id)\n",
    "clusters_train = k_means.k_means(new_SparseVec_train, num_tokens, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covert_to_word_vector(vector, dictionary):    \n",
    "    new_word_doc = []\n",
    "    for tup in vector:\n",
    "        word = dictionary.get(tup[0]) # retrieve word from old_dictionary          \n",
    "        new_doc.append(word,tup[1])\n",
    "    return new_word_doc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cluster_index = 0\n",
    "for cluster in clusters_train:\n",
    "    # Cluster contains a list of index of new_SparseVec_train\n",
    "    \n",
    "    cluster_tfidf = [] \n",
    "    cluster_words = [] \n",
    "    for sparseVec_index in cluster:\n",
    "        word_vec = covert_to_word_vector(new_SparseVec_train[sparseVec_index], new_dict_train)\n",
    "        for tup in word_vec: \n",
    "            if tup not in cluster_tfidf:\n",
    "                cluster_tfidf.append(tup)\n",
    "            cluster_words.append(tup[0])\n",
    "\n",
    "    cluster_tfidf.sort(key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    cluster_words = [[word,cluster_words.count(word)] for word in set(cluster_words)]\n",
    "    cluster_words.sort(key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Cluster \", cluster_index,\" :\")\n",
    "    print(\"Top TF-IDF Weighted Words :\", cluster_tfidf[:20], )\n",
    "    print(\"Top Frequency :\", cluster_words[:20], '\\n')\n",
    "    cluster_index += 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.4 Document Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.4.1 Classifying Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Laballed Dictionary for Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_data = []\n",
    "\n",
    "cluster_index = 0\n",
    "for cluster in clusters_train:\n",
    "    cluster_dict = {}\n",
    "    for sparseVec_index in cluster:\n",
    "        for tup in new_SparseVec_train[sparseVec_index]: \n",
    "            if tup[0] not in cluster_dict.keys():\n",
    "                cluster_dict[tup[0]] = 1 # Freq is 1 since this value does not matter - Lab 5 \n",
    "    \n",
    "    cluster_name = 'Topic ' + str(cluster_index)\n",
    "    all_labeled_data.append((cluster_dict, cluster_name))\n",
    "    \n",
    "    cluster_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(all_labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = {}\n",
    "\n",
    "for tup in new_SparseVec_train[3]:  \n",
    "    if tup[0] not in text_dict.keys():\n",
    "        text_dict[tup[0]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n"
     ]
    }
   ],
   "source": [
    "print(classifier.classify(text_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'hereh.', 'eh', 'h', 'heh', 'e', 'heh']\n"
     ]
    }
   ],
   "source": [
    "# Transcript to BOW\n",
    "def get_result(transcript):\n",
    "    print(transcript.split())\n",
    "    #dict_train.doc2bow(doc)\n",
    "\n",
    "get_result(\"he hereh. eh h heh e heh \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.4.2 Classifying Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Training Corpus and Pre-Processing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowVec_test = [new_dict_train.doc2bow(doc) for doc in testData_processed]\n",
    "tfidfModel_test = gensim.models.TfidfModel(bowVec_test) \n",
    "SparseVec_test = [tfidfModel_test[vec] for vec in bowVec_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_SparseVec_test = Extract_Doc_Top_N_Tfidf_Words(SparseVec_test, 50) \n",
    "\n",
    "# Convert documents into dict representation.\n",
    "testData_as_dict = [{id:1 for (id, tf_value) in vec} for vec in new_SparseVec_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each file, classify and print the label.\n",
    "for i in range(len(test_fids)):\n",
    "    print(test_fids[i], '-->', classifier.classify(testData_as_dict[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "# DOCUMENT RETRIEVAL????\n",
    "similarity_index = similarities.SparseMatrixSimilarity(vecs1, len(dictionary))\n",
    "\n",
    "test_vector = vecs1[100]\n",
    "sims = similarity_index[test_vector]\n",
    "sorted_sims = sorted(enumerate(sims), key = lambda item: -item[1])\n",
    "\n",
    "print(list(enumerate(sorted_sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archieved - Ignore the Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import string\n",
    "import kmeans\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "metadata_dataset = pd.read_csv(\"../Processed_Dataset/cleaned_dataset.csv\", keep_default_na=False, na_values=[\"\"])\n",
    "\n",
    "data = metadata_dataset['transcript']\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features = 8000,\n",
    "    stop_words = 'english',\n",
    "     \n",
    "\n",
    "tfidf.fit(data)\n",
    "text = tfidf.transform(data)\n",
    "\n",
    "print(type(tfid))\n",
    "\n",
    "# def find_optimal_clusters(data, max_k):\n",
    "#     iters = range(2, max_k+1, 2)\n",
    "    \n",
    "#     sse = []\n",
    "#     for k in iters:\n",
    "#         sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n",
    "#         print('Fit {} clusters'.format(k))\n",
    "        \n",
    "#     f, ax = plt.subplots(1, 1)\n",
    "#     ax.plot(iters, sse, marker='o')\n",
    "#     ax.set_xlabel('Cluster Centers')\n",
    "#     ax.set_xticks(iters)\n",
    "#     ax.set_xticklabels(iters)\n",
    "#     ax.set_ylabel('SSE')\n",
    "#     ax.set_title('SSE by Cluster Center Plot')\n",
    "    \n",
    "# find_optimal_clusters(text, 150)\n",
    "\n",
    "#vecs1 = [dictionary.doc2bow(doc) for doc in docs5]\n",
    "\n",
    "#tf_idf = vecs1.fit_transform(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast = load_breast_cancer()\n",
    "breast_data = breast.data\n",
    "\n",
    "print(breast_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
