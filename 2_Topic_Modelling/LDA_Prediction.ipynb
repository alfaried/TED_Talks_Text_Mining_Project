{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing of Libraries\n",
    "Run this as a seperate cell in order to reduce latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing of external library; mallet\n",
    "mallet_path = r'/Library/NLTK/mallet/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets list of stop words and stemmer object\n",
    "stop_list = nltk.corpus.stopwords.words(\"english\")\n",
    "stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts text files into a corpus\n",
    "def corpus2docs(corpus):\n",
    "    fids = corpus.fileids()\n",
    "    docs1 = []\n",
    "    for fid in fids:\n",
    "        doc_raw = corpus.raw(fid)\n",
    "        doc = nltk.word_tokenize(doc_raw)\n",
    "        docs1.append(doc)\n",
    "    docs2 = [[w.lower() for w in doc] for doc in docs1]\n",
    "    docs3 = [[w for w in doc if re.search('^[a-z]+$', w)] for doc in docs2]\n",
    "    docs4 = [[w for w in doc if w not in stop_list] for doc in docs3]\n",
    "    docs5 = [[stemmer.stem(w) for w in doc] for doc in docs4]\n",
    "    return docs5, fids\n",
    "\n",
    "# Converts docs into vectors\n",
    "def docs2vecs(docs, dictionary):\n",
    "    vecs1 = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    return vecs1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of Train Dataset\n",
    "Load and preprocess 1872 training dataset for dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating of dictionary\n",
    "train_corpus = nltk.corpus.PlaintextCorpusReader(\"./TrainTest_Transcripts/Train/\", \".+\\.txt\")\n",
    "train_docs, train_fids = corpus2docs(train_corpus)\n",
    "dictionary = gensim.corpora.Dictionary(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of Test Dataset\n",
    "Load and preprocess 468 testing dataset for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating of the test vectors for prediciton\n",
    "test_corpus = nltk.corpus.PlaintextCorpusReader(\"./TrainTest_Transcripts/Test/\", \".+\\.txt\")\n",
    "test_docs, test_fids = corpus2docs(test_corpus)\n",
    "test_vecs = docs2vecs(test_docs, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of LDA Model\n",
    "Load LDA model with N topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_topics = 48\n",
    "\n",
    "# Models are in increment of 2, from 2 to 120. Choose and even number model during loading\n",
    "lda_model = gensim.models.wrappers.LdaMallet.load(\"./LDA_Models/train_LDA_\" + str(no_of_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.mallet_path = mallet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Library/NLTK/mallet/mallet-2.0.8/bin/mallet'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.mallet_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of Topics\n",
    "The prediction of the distribtuion of topics for the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, data):\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(data)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '/Library/NLTK/mallet/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input /var/folders/gf/h7z28hjn1bsfqnvmxnhjt5y80000gn/T/a3ba6f_corpus.txt --output /var/folders/gf/h7z28hjn1bsfqnvmxnhjt5y80000gn/T/a3ba6f_corpus.mallet.infer --use-pipe-from /var/folders/gf/h7z28hjn1bsfqnvmxnhjt5y80000gn/T/a3ba6f_corpus.mallet' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-75d6c524e241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_docs_topics_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_topics_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-8df298564df6>\u001b[0m in \u001b[0;36mformat_topics_sentences\u001b[0;34m(ldamodel, corpus, data)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Get main topic in each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, bow, iterations)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmallet_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' infer-topics --input %s --inferencer %s '\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36mconvert_input\u001b[0;34m(self, corpus, infer, serialize_corpus)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcorpustxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcorpusmallet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"converting temporary corpus to MALLET format with %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '/Library/NLTK/mallet/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input /var/folders/gf/h7z28hjn1bsfqnvmxnhjt5y80000gn/T/a3ba6f_corpus.txt --output /var/folders/gf/h7z28hjn1bsfqnvmxnhjt5y80000gn/T/a3ba6f_corpus.mallet.infer --use-pipe-from /var/folders/gf/h7z28hjn1bsfqnvmxnhjt5y80000gn/T/a3ba6f_corpus.mallet' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "df_docs_topics_distribution = format_topics_sentences(ldamodel=lda_model, corpus=test_vecs, data=test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_docs_topics_distribution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-d056b6158c3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_docs_dominant_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_docs_topics_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_docs_dominant_topic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Transcript_No'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dominant_Topic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Topic_Perc_Contrib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Keywords'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Transcript'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Find the true transcript number from the entire corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_docs_topics_distribution' is not defined"
     ]
    }
   ],
   "source": [
    "# Format\n",
    "df_docs_dominant_topic = df_docs_topics_distribution.reset_index()\n",
    "df_docs_dominant_topic.columns = ['Transcript_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Transcript']\n",
    "\n",
    "# Find the true transcript number from the entire corpus\n",
    "transcript_no_hash = {}\n",
    "for index in df_docs_dominant_topic[\"Transcript_No\"]:\n",
    "    transcript_no = test_fids[index].split(\".\")[0].split(\"_\")[1]\n",
    "    transcript_no_hash[index] = transcript_no\n",
    "\n",
    "# Replace data in \"index\" column\n",
    "df_docs_dominant_topic[\"Transcript_No\"].replace(transcript_no_hash, inplace=True)\n",
    "\n",
    "# Show\n",
    "df_docs_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transcript_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1132</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[good, morn, laughter, great, blown, away, who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1678</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[want, afternoon, someth, littl, differ, sched...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2485</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[everyon, need, coach, matter, whether, basket...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1687</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[teach, chemistri, explos, right, right, explo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1934</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[littl, nervou, wife, yvonn, said, said, geoff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1159</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[welcom, five, danger, thing, let, children, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[first, children, book, publish, return, old, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[kindergarten, design, made, kindergarten, cir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[thank, much, everyon, ted, chri, ami, particu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1545</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[today, go, show, tablet, headset, wear, go, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2740</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[know, set, parent, expert, fact, interest, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[rememb, first, time, went, nice, restaur, rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>2194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2052</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[want, talk, kid, know, everyon, think, kid, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>2203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1463</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[decad, studi, young, peopl, push, school, cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1663</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[wrote, letter, last, week, talk, work, founda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1702</td>\n",
       "      <td>school, kid, learn, student, children, educ, t...</td>\n",
       "      <td>[want, start, question, last, time, call, chil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Transcript_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0               0             0.0              0.1132   \n",
       "74           1317             0.0              0.1678   \n",
       "86           1390             0.0              0.2485   \n",
       "87           1391             0.0              0.1687   \n",
       "88           1394             0.0              0.1934   \n",
       "131           163             0.0              0.1159   \n",
       "141          1691             0.0              0.1582   \n",
       "177          1842             0.0              0.1361   \n",
       "198           193             0.0              0.1354   \n",
       "227          2083             0.0              0.1545   \n",
       "247          2159             0.0              0.2740   \n",
       "249          2168             0.0              0.1140   \n",
       "257          2194             0.0              0.2052   \n",
       "262          2203             0.0              0.1463   \n",
       "331           364             0.0              0.1663   \n",
       "378           616             0.0              0.1702   \n",
       "\n",
       "                                              Keywords  \\\n",
       "0    school, kid, learn, student, children, educ, t...   \n",
       "74   school, kid, learn, student, children, educ, t...   \n",
       "86   school, kid, learn, student, children, educ, t...   \n",
       "87   school, kid, learn, student, children, educ, t...   \n",
       "88   school, kid, learn, student, children, educ, t...   \n",
       "131  school, kid, learn, student, children, educ, t...   \n",
       "141  school, kid, learn, student, children, educ, t...   \n",
       "177  school, kid, learn, student, children, educ, t...   \n",
       "198  school, kid, learn, student, children, educ, t...   \n",
       "227  school, kid, learn, student, children, educ, t...   \n",
       "247  school, kid, learn, student, children, educ, t...   \n",
       "249  school, kid, learn, student, children, educ, t...   \n",
       "257  school, kid, learn, student, children, educ, t...   \n",
       "262  school, kid, learn, student, children, educ, t...   \n",
       "331  school, kid, learn, student, children, educ, t...   \n",
       "378  school, kid, learn, student, children, educ, t...   \n",
       "\n",
       "                                            Transcript  \n",
       "0    [good, morn, laughter, great, blown, away, who...  \n",
       "74   [want, afternoon, someth, littl, differ, sched...  \n",
       "86   [everyon, need, coach, matter, whether, basket...  \n",
       "87   [teach, chemistri, explos, right, right, explo...  \n",
       "88   [littl, nervou, wife, yvonn, said, said, geoff...  \n",
       "131  [welcom, five, danger, thing, let, children, c...  \n",
       "141  [first, children, book, publish, return, old, ...  \n",
       "177  [kindergarten, design, made, kindergarten, cir...  \n",
       "198  [thank, much, everyon, ted, chri, ami, particu...  \n",
       "227  [today, go, show, tablet, headset, wear, go, c...  \n",
       "247  [know, set, parent, expert, fact, interest, pa...  \n",
       "249  [rememb, first, time, went, nice, restaur, rea...  \n",
       "257  [want, talk, kid, know, everyon, think, kid, f...  \n",
       "262  [decad, studi, young, peopl, push, school, cal...  \n",
       "331  [wrote, letter, last, week, talk, work, founda...  \n",
       "378  [want, start, question, last, time, call, chil...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_docs_dominant_topic.loc[df_docs_dominant_topic[\"Dominant_Topic\"] == 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
